{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will build a character-level text autocomplete model using a Recurrent Neural Network (RNN) in PyTorch. We will train the model on the text from \"warandpeace.txt\". This project will help you understand how RNNs can be implemented for text generation tasks and their application in building your own autocomplete model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# This is Cell #2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing the Data\n",
    "\n",
    "Now it is time to prepare our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #3\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read().lower()\n",
    "        # Keep only lowercase letters and standard punctuation (.,!?;:()[])\n",
    "        text = re.sub(r'[^a-z.,!?;:()\\[\\] ]+', '', text)\n",
    "    return text\n",
    "\n",
    "# sequence = read_file(\"warandpeace.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will train our model with a simple sequence\n",
    "\n",
    "We will start by training our model with a simple sequence and repettitive sequence such as `\"abcdefghijklmnopqrstuvwxyzabcdef...\"`, and we will see if our RNN is capable of learning that pattern or not. This will help you easily verify if your RNN is working correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #4\n",
    "\n",
    "sequence = \"abcdefghijklmnopqrstuvwxyz\" * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Character Mappings\n",
    "\n",
    "Creating character mappings is essential because RNNs require numerical input to process data. By mapping each unique character to an index and creating a reverse mapping, we convert text data into numerical sequences that the model can understand. This step allows us to encode input text for training and decode the model's output back into readable characters during text generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #5\n",
    "\n",
    "#TODO: Create a list of unique characters from the text sequence\n",
    "vocab = \n",
    "\n",
    "#TODO: Create two dictionaries for character-index mappings that map each character in vocab to a unique index and vice versa\n",
    "char_to_idx = \n",
    "idx_to_char = \n",
    "\n",
    "#TODO: Convert the entire text based data into numerical data\n",
    "data = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CharDataset Class\n",
    "\n",
    "Now we will create a custom dataset class to generate sequences and targets for training\n",
    "\n",
    "Creating a custom `CharDataset` class is crucial because it prepares our text data into input sequences and target sequences that the RNN can learn from. By organizing the data this way, we can efficiently feed batches of sequences into the model during training, allowing it to learn the patterns of character sequences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #6\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, stride, vocab_size):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Create overlapping sequences with stride\n",
    "        for i in range(0, len(data) - sequence_length, stride):\n",
    "            self.sequences.append(data[i:i + sequence_length])\n",
    "            self.targets.append(data[i + 1:i + sequence_length + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return sequence, target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters\n",
    "\n",
    "Now we will set our model's hyperparameters for our training process\n",
    "\n",
    "Setting hyperparameters is important because they define the model's architecture and training behavior. They determine how the RNN processes data, learns patterns, and how quickly it converges during training. Properly chosen hyperparameters can significantly improve model performance and is a key step in training of models\n",
    "\n",
    "Set the following hyperparameters for your model in the code cell below:\n",
    "`sequence_length`, `stride`, `embedding_dim`, `hidden_size`, `num_layers`, `learning_rate`, `num_epochs`, `batch_size`, `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #7\n",
    "\n",
    "#TODO: Set your model's hyperparameters\n",
    "\n",
    "sequence_length = 1000  # Length of each input sequence\n",
    "stride = 10            # Stride for creating sequences\n",
    "embedding_dim = 2     # Dimension of character embeddings\n",
    "hidden_size = 1      # Number of features in the hidden state of the RNN\n",
    "learning_rate = 200  # Learning rate for the optimizer\n",
    "num_epochs = 1         # Number of epochs to train\n",
    "batch_size = 64        # Batch size for training\n",
    "vocab_size = len(vocab)\n",
    "input_size = len(vocab)\n",
    "output_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have set your hyperparameters in the code cell above, very breifly tell what is the role of each of the hyperparameter that you have defined above.\n",
    "\n",
    "TODO: Explain below\n",
    "> Here\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets\n",
    "\n",
    "By now at this point in class, I'm confident that you know why we do this, so I'm not gonna say a lot here, let's jump right into the todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #8\n",
    "\n",
    "data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "#TODO: Convert the data into a pytorch tensor and split the data into 90:10 ratio\n",
    "train_size = \n",
    "train_data = \n",
    "test_data = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders\n",
    "\n",
    "Now we will create data loaders for easy batching during training and testing.\n",
    "\n",
    "Creating data loaders is essential to batch the data during training and testing. Batching allows the RNN to process multiple sequences in parallel, which speeds up training and makes better use of computational resources. \n",
    "We will also use Data loaders to shuffle the batched data, which is important for training models that generalize well.\n",
    "\n",
    "Make sure to set `drop_last=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #9\n",
    "\n",
    "train_dataset = CharDataset(train_data, sequence_length, stride, vocab_size)\n",
    "test_dataset = CharDataset(test_data, sequence_length, stride, vocab_size)\n",
    "\n",
    "#TODO: Initialize the training and testing data loader with batching and shuffling equal to True for training (and shuffling = False for testing)\n",
    "train_loader = \n",
    "test_loader = \n",
    "\n",
    "total_batches = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RNN Model\n",
    "\n",
    "Here we will define our character-level RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #10\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim=30):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(output_size, embedding_dim)\n",
    "        self.W_e = nn.Parameter(torch.randn(hidden_size, embedding_dim) * 0.01)  # Smaller std\n",
    "        self.b_e = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)  # Smaller std\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size)) \n",
    "        #TODO: set the fully connected layer\n",
    "        self.fc = \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x in [b, l] # b is batch_size and l is sequence length\n",
    "        \"\"\"\n",
    "        x_embed = self.embedding(x)  # [b=batch_size, l=sequence_length, e=embedding_dim]\n",
    "        b, l, _ = x_embed.size()\n",
    "        x_embed = x_embed.transpose(0, 1) # [l, b, e]\n",
    "        if hidden is None:\n",
    "            h_t_minus_1 = self.init_hidden(b)\n",
    "        else:\n",
    "            h_t_minus_1 = hidden\n",
    "        output = []\n",
    "        for t in range(l):\n",
    "            # RNN equation from the lecture \n",
    "            # We add a bias as well to expand the range of learnable functions\n",
    "            h_t = torch.tanh(x_embed[t] @ self.W_e.T + self.b_e + h_t_minus_1 @ self.W_h.T + self.b_h) # [b, e]\n",
    "            output.append(h_t)\n",
    "            h_t_minus_1 = h_t\n",
    "        output = torch.stack(output) # [l, b, e]\n",
    "        output = output.transpose(0, 1) # [b, l, e]\n",
    "        final_hidden = h_t.clone() # [b, h]\n",
    "        logits = self.fc(output) # [b, l, vocab_size=v] \n",
    "        return logits, final_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a basic high level understanding of what is the CharRNN model that you just defined above, it consists of an embedding layer, an RNN layer, and a fully connected layer. Then embedding layer converts character indices into embeddings. Then RNN processes the embeddings and captures sequential information. Then finally the fully connected layer maps the RNN outputs to the vocabulary size for character prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Model, Loss Function, and Optimizer\n",
    "\n",
    "Now we will create an instance of the model that we just defined above and set up the loss function and optimizer. Then we will define a loss function, that evaluates the model's prediction against the true targets, and attaches a cost (number) on how good/bad the model is doing. During our training process, it is this cost that we try to minimize by tweaking the weights of the network. \n",
    "\n",
    "Then we will set up an optimizer, which will update the model's parameters based on the loss returned by the our loss function. This is how our model will learn over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #12\n",
    "\n",
    "#TODO: Initialize your RNN model\n",
    "model = \n",
    "\n",
    "#TODO: Define the loss function (use cross entropy loss)\n",
    "criterion = \n",
    "\n",
    "#TODO: Initialize your optimizer passing your model parameters and training hyperparameters\n",
    "optimizer = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now finally, after all the setup that we have done, we can train our RNN. \n",
    "\n",
    "A basic idea high level idea of what we will do here is we will loop over epochs and batches to train the model. \n",
    "We will Initialize the hidden state at the beginning of each epoch. For each batch, we will reset the gradients, perform a forward pass, compute the loss, perform backpropagation, and update the model parameters. Then we detach the hidden state to prevent gradients from backpropagating through previous batches. We ill repeat this process for each batch. And finally we will calculate the average loss and accuracy for each epoch.\n",
    "By performing forward and backward passes, calculating loss, and updating the model parameters, we enable the RNN to improve its predictions with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #13\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss, correct_predictions, total_predictions = 0, 0, 0\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    for batch_idx, (batch_inputs, batch_targets) in tqdm(enumerate(train_loader), total=total_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "        output, hidden = model(batch_inputs, hidden)\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        loss = criterion(output.view(-1, output_size), batch_targets.view(-1))  # Flatten the outputs and targets for CrossEntropyLoss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate accuracy\n",
    "            _, predicted_indices = torch.max(output, dim=2)  # Predicted characters\n",
    "\n",
    "            correct_predictions += (predicted_indices == batch_targets).sum().item()\n",
    "            total_predictions += batch_targets.size(0) * batch_targets.size(1)  # Total items in this batch\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100  # Convert to percentage\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your loss\n",
    "\n",
    "The training loss of your model when trained with a simple sequence like `\"abcdefghijklmnopqrstuvwxyz\" * 100` should be extremely close to zero. If that's not the case, go back and fix your bugs ;)\n",
    "\n",
    "If you have acheived a training loss of 0 or extremley close to 0, then congratulations, lets move on to train your model with a bit more complicated sequence. That is our old favorite book, `warandpeace.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the `warandpeace.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #14\n",
    "\n",
    "sequence = read_file('warandpeace.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Follow the instructions\n",
    "\n",
    "1. Re-run Cell #5 to re-create character mappings for `warandpeace.txt`\n",
    "2. Re-run Cell #7 to re-initialize hyperparameters\n",
    "3. Re-run Cell #8 to split and create training and testing data with `warandpeace.txt` as your corpus\n",
    "4. Re-run Cell #9 to set up data loaders with `warandpeace.txt` data\n",
    "5. Re-run Cell #12 to re-initialize a new model object (maybe ask yourself why can't you use the previous model that was trained on the simple `\"abc...\"` corpus)\n",
    "6. Re-run Cell #13 to train the new model with `warandpeace.txt` data.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "After training, we evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #15\n",
    "\n",
    "with torch.no_grad():\n",
    "    #TODO: Write the testing loop for your trained model by refering to the training loop code given to you above\n",
    "\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with the Trained Model\n",
    "\n",
    "In this part of the assignment, your task is to implement the `generate_text` function, which uses a trained RNN model to generate text character-by-character, continuing from a given input. The function will produce an extended sequence by repeatedly predicting and appending the next character to the input.\n",
    "\n",
    "### What the function is supposed to do?\n",
    "\n",
    "1. Take an initial input text of length `n` from the user, convert it into indices using a predefined vocabulary (char_to_idx).\n",
    "2. Use a trained model to predict the next character in the sequence.\n",
    "3. Append the predicted character to the input, extend the input sequence, and repeat the process until `k` additional characters are generated.\n",
    "4. Return the generated text, including the original input and the newly predicted characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #16\n",
    "\n",
    "def sample_from_output(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample from the logits with temperature scaling.\n",
    "    logits: Tensor of shape [batch_size, vocab_size] (raw scores, before softmax)\n",
    "    temperature: a float controlling the randomness (higher = more random)\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling to logits (increase randomness with higher values)\n",
    "    scaled_logits = logits / temperature  # Scale the logits by temperature\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = F.softmax(scaled_logits, dim=1)\n",
    "    \n",
    "    # Sample from the probability distribution\n",
    "    sampled_idx = torch.multinomial(probabilities, 1)  # Sample one index from the probability distribution\n",
    "    return sampled_idx\n",
    "\n",
    "def generate_text(model, start_text, n, k, temperature=1.0):\n",
    "    \"\"\"\n",
    "        model: The trained RNN model used for character prediction.\n",
    "        start_text: The initial string of length `n` provided by the user to start the generation.\n",
    "        n: The length of the initial input sequence.\n",
    "        k: The number of additional characters to generate.\n",
    "        temperature: Optional\n",
    "        A scaling factor for randomness in predictions. Higher values (e.g., >1) make \n",
    "            predictions more random, while lower values (e.g., <1) make predictions more deterministic.\n",
    "            Default is 1.0.\n",
    "    \"\"\"\n",
    "    start_text = start_text.lower()\n",
    "    #TODO: Implement the rest of the generate_text function\n",
    "\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "print(\"Training complete. Now you can generate text.\")\n",
    "while True:\n",
    "    start_text = input(\"Enter the initial text (n characters, or 'exit' to quit): \")\n",
    "    \n",
    "    if start_text.lower() == 'exit':\n",
    "        print(\"Exiting...\")\n",
    "        break\n",
    "    \n",
    "    n = len(start_text) \n",
    "    k = int(input(\"Enter the number of characters to generate: \"))\n",
    "    temperature_input = input(\"Enter the temperature value (1.0 is default, >1 is more random): \")\n",
    "    temperature = float(temperature_input) if temperature_input else 1.0\n",
    "    \n",
    "    completed_text = generate_text(model, start_text, n, k, temperature)\n",
    "    \n",
    "    print(f\"Generated text: {completed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report section\n",
    "\n",
    "In your report, describe your experiments and observations when training the model with two datasets: (1) the sequence `\"abcdefghijklmnopqrstuvwxyz\" * 100` and (2) the text from `warandpeace.txt`. Include the final loss values for both datasets and discuss how the generated text differed between the two. Explain the impact of changing the `temperature` parameter on the text generation, and provide examples. Reflect on the challenges you faced, your thought process during implementation, and the key insights you gained about RNNs and sequence modeling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
