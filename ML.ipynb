{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:  I was training on my laptop and on Google Colab at the same time to speed things up. The training logs for the warandpeace_models (accuracy and logs) are in ML2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will build a character-level text autocomplete model using a Recurrent Neural Network (RNN) in PyTorch. We will train the model on the text from \"warandpeace.txt\". This project will help you understand how RNNs can be implemented for text generation tasks and their application in building your own autocomplete model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# This is Cell #2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing the Data\n",
    "\n",
    "Now it is time to prepare our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #3\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read().lower()\n",
    "        # Keep only lowercase letters and standard punctuation (.,!?;:()[])\n",
    "        text = re.sub(r'[^a-z.,!?;:()\\[\\] ]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will train our model with a simple sequence\n",
    "\n",
    "We will start by training our model with a simple sequence and repettitive sequence such as `\"abcdefghijklmnopqrstuvwxyzabcdef...\"`, and we will see if our RNN is capable of learning that pattern or not. This will help you easily verify if your RNN is working correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #4\n",
    "sequence = \"abcdefghijklmnopqrstuvwxyz\" * 100\n",
    "model_path = './basic_alphabet_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = read_file('warandpeace.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Character Mappings\n",
    "\n",
    "Creating character mappings is essential because RNNs require numerical input to process data. By mapping each unique character to an index and creating a reverse mapping, we convert text data into numerical sequences that the model can understand. This step allows us to encode input text for training and decode the model's output back into readable characters during text generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #5\n",
    "\n",
    "#TODO: Create a list of unique characters from the text sequence\n",
    "vocab = list(set(sequence))\n",
    "\n",
    "#TODO: Create two dictionaries for character-index mappings that map each character in vocab to a unique index and vice versa\n",
    "char_to_idx = {vocab[i]: i for i in range(len(vocab))}\n",
    "idx_to_char = {i: vocab[i] for i in range(len(vocab))}\n",
    "\n",
    "#TODO: Convert the entire text based data into numerical data\n",
    "data = [char_to_idx[char] for char in sequence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./basic_alphabet_models/char_to_idx.pkl', 'wb') as f:\n",
    "  pickle.dump(char_to_idx, f)\n",
    "\n",
    "with open('./basic_alphabet_models/idx_to_char.pkl', 'wb') as f:\n",
    "  pickle.dump(idx_to_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./warandpeace_models/char_to_idx.pkl\", 'rb') as f:\n",
    "    char_to_idx = pickle.load(f)\n",
    "\n",
    "with open(\"./warandpeace_models/idx_to_char.pkl\", 'rb') as f:\n",
    "    idx_to_char = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CharDataset Class\n",
    "\n",
    "Now we will create a custom dataset class to generate sequences and targets for training\n",
    "\n",
    "Creating a custom `CharDataset` class is crucial because it prepares our text data into input sequences and target sequences that the RNN can learn from. By organizing the data this way, we can efficiently feed batches of sequences into the model during training, allowing it to learn the patterns of character sequences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #6\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, stride, vocab_size):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Create overlapping sequences with stride\n",
    "        for i in range(0, len(data) - sequence_length, stride):\n",
    "            self.sequences.append(data[i:i + sequence_length])\n",
    "            self.targets.append(data[i + 1:i + sequence_length + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return sequence, target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters\n",
    "\n",
    "Now we will set our model's hyperparameters for our training process\n",
    "\n",
    "Setting hyperparameters is important because they define the model's architecture and training behavior. They determine how the RNN processes data, learns patterns, and how quickly it converges during training. Properly chosen hyperparameters can significantly improve model performance and is a key step in training of models\n",
    "\n",
    "Set the following hyperparameters for your model in the code cell below:\n",
    "`sequence_length`, `stride`, `embedding_dim`, `hidden_size`, `num_layers`, `learning_rate`, `num_epochs`, `batch_size`, `vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #7\n",
    "\n",
    "#TODO: Set your model's hyperparameters\n",
    "\n",
    "sequence_length = 100  # Length of each input sequence\n",
    "stride = 10            # Stride for creating sequences\n",
    "embedding_dim = 30     # Dimension of character embeddings\n",
    "hidden_size = 500     # Number of features in the hidden state of the RNN\n",
    "learning_rate = 1e-3  # Learning rate for the optimizer\n",
    "num_epochs = 10        # Number of epochs to train\n",
    "batch_size = 64        # Batch size for training\n",
    "vocab_size = len(vocab)\n",
    "input_size = len(vocab)\n",
    "output_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have set your hyperparameters in the code cell above, very breifly tell what is the role of each of the hyperparameter that you have defined above.\n",
    "\n",
    "TODO: Explain below\n",
    "> Sequence length: It determines the number of previous character embeddings that are used in calculating the hidden layer. \n",
    "> \n",
    "> Stride: It determines the separation between two sequences\n",
    "> \n",
    "> Embedding_dim: It determines the dimensions of character embeddings.\n",
    "> \n",
    "> Hidden_size: It determines the dimensions/num features of the hidden state that contains the context prior to the current character.\n",
    "> \n",
    "> Learning rate: It determines how much each gradient descent/optimization step affects the model.\n",
    "> \n",
    "> Num_epochs: It determines how many times the model trains through all the batches.\n",
    "> \n",
    "> Batch_size: It determines the amount of data that is fed into the model for forward propagation before a step of optimization.\n",
    "> \n",
    "> Vocab_size: The number of distinguish characters in the large sequence of text.\n",
    "> \n",
    "> Input_size: The size of the dictionary of embeddings, same as vocab size in this case\n",
    "> \n",
    "> Output_size: The size of the array that contains the score for each character in the vocab, same as vocab size in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets\n",
    "\n",
    "By now at this point in class, I'm confident that you know why we do this, so I'm not gonna say a lot here, let's jump right into the todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #8\n",
    "\n",
    "data_tensor = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "#TODO: Convert the data into a pytorch tensor and split the data into 90:10 ratio\n",
    "train_size = int(data_tensor.size(0) * 0.9)\n",
    "train_data, test_data = data_tensor.split([train_size, data_tensor.size(0) - train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders\n",
    "\n",
    "Now we will create data loaders for easy batching during training and testing.\n",
    "\n",
    "Creating data loaders is essential to batch the data during training and testing. Batching allows the RNN to process multiple sequences in parallel, which speeds up training and makes better use of computational resources. \n",
    "We will also use Data loaders to shuffle the batched data, which is important for training models that generalize well.\n",
    "\n",
    "Make sure to set `drop_last=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #9\n",
    "\n",
    "train_dataset = CharDataset(train_data, sequence_length, stride, vocab_size)\n",
    "test_dataset = CharDataset(test_data, sequence_length, stride, vocab_size)\n",
    "\n",
    "#TODO: Initialize the training and testing data loader with batching and shuffling equal to True for training (and shuffling = False for testing)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "\n",
    "total_batches = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the RNN Model\n",
    "\n",
    "Here we will define our character-level RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #10\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim=30):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_dim)\n",
    "        self.W_e = nn.Parameter(torch.randn(hidden_size, embedding_dim) * 0.01)  # Smaller std\n",
    "        self.b_e = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)  # Smaller std\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size)) \n",
    "        #TODO: set the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x in [b, l] # b is batch_size and l is sequence length\n",
    "        \"\"\"\n",
    "        x_embed = self.embedding(x)  # [b=batch_size, l=sequence_length, e=embedding_dim]\n",
    "        b, l, _ = x_embed.size()\n",
    "        x_embed = x_embed.transpose(0, 1) # [l, b, e]\n",
    "        if hidden is None:\n",
    "            h_t_minus_1 = self.init_hidden(b)\n",
    "        else:\n",
    "            h_t_minus_1 = hidden\n",
    "        output = []\n",
    "        for t in range(l):\n",
    "            # RNN equation from the lecture \n",
    "            # We add a bias as well to expand the range of learnable functions\n",
    "            h_t = torch.tanh(x_embed[t] @ self.W_e.T + self.b_e + h_t_minus_1 @ self.W_h.T + self.b_h) # [b, e]\n",
    "            output.append(h_t)\n",
    "            h_t_minus_1 = h_t\n",
    "        output = torch.stack(output) # [l, b, e]\n",
    "        output = output.transpose(0, 1) # [b, l, e]\n",
    "        final_hidden = h_t.clone() # [b, h]\n",
    "        logits = self.fc(output) # [b, l, vocab_size=v] \n",
    "        return logits, final_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a basic high level understanding of what is the CharRNN model that you just defined above, it consists of an embedding layer, an RNN layer, and a fully connected layer. Then embedding layer converts character indices into embeddings. Then RNN processes the embeddings and captures sequential information. Then finally the fully connected layer maps the RNN outputs to the vocabulary size for character prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Model, Loss Function, and Optimizer\n",
    "\n",
    "Now we will create an instance of the model that we just defined above and set up the loss function and optimizer. Then we will define a loss function, that evaluates the model's prediction against the true targets, and attaches a cost (number) on how good/bad the model is doing. During our training process, it is this cost that we try to minimize by tweaking the weights of the network. \n",
    "\n",
    "Then we will set up an optimizer, which will update the model's parameters based on the loss returned by the our loss function. This is how our model will learn over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #12\n",
    "\n",
    "#TODO: Initialize your RNN model\n",
    "model = CharRNN(input_size, hidden_size, output_size, embedding_dim)\n",
    "\n",
    "#TODO: Define the loss function (use cross entropy loss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#TODO: Initialize your optimizer passing your model parameters and training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now finally, after all the setup that we have done, we can train our RNN. \n",
    "\n",
    "A basic idea high level idea of what we will do here is we will loop over epochs and batches to train the model. \n",
    "We will Initialize the hidden state at the beginning of each epoch. For each batch, we will reset the gradients, perform a forward pass, compute the loss, perform backpropagation, and update the model parameters. Then we detach the hidden state to prevent gradients from backpropagating through previous batches. We ill repeat this process for each batch. And finally we will calculate the average loss and accuracy for each epoch.\n",
    "By performing forward and backward passes, calculating loss, and updating the model parameters, we enable the RNN to improve its predictions with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40:   0%|          | 0/2 [00:00<?, ?it/s]/var/folders/jx/gpl581kj3ps18jk73ly90pt80000gn/T/ipykernel_50240/1273917643.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
      "/var/folders/jx/gpl581kj3ps18jk73ly90pt80000gn/T/ipykernel_50240/1273917643.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
      "Epoch 1/40: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Loss: 1009.3695, Accuracy: 5.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Loss: 500.1909, Accuracy: 71.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40], Loss: 241.7917, Accuracy: 86.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40], Loss: 134.7996, Accuracy: 88.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40], Loss: 6.2631, Accuracy: 98.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40], Loss: 0.2043, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40], Loss: 84.4335, Accuracy: 96.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40], Loss: 120.9919, Accuracy: 96.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40], Loss: 11.8734, Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40], Loss: 0.0001, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40], Loss: 0.0018, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40], Loss: 322.1545, Accuracy: 96.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40], Loss: 62.7271, Accuracy: 96.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40], Loss: 6.7360, Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40], Loss: 0.9317, Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40], Loss: 0.1089, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40], Loss: 18.2478, Accuracy: 96.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40], Loss: 0.0387, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40], Loss: 0.0450, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40], Loss: 0.0112, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40], Loss: 0.0049, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40], Loss: 0.0380, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40], Loss: 0.0446, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40], Loss: 0.0272, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/40], Loss: 2.3468, Accuracy: 98.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/40], Loss: 0.0020, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40], Loss: 0.0239, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/40], Loss: 0.0052, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/40], Loss: 0.0147, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|██████████| 2/2 [00:01<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/40], Loss: 0.0026, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/40], Loss: 0.0016, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/40], Loss: 0.0010, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/40], Loss: 0.0106, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|██████████| 2/2 [00:01<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/40], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3227cfb90>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is Cell #13\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss, correct_predictions, total_predictions = 0, 0, 0\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch_idx, (batch_inputs, batch_targets) in tqdm(enumerate(train_loader), total=total_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "        output, hidden = model(batch_inputs, hidden)\n",
    "        hidden = hidden.detach()\n",
    "        #print([idx_to_char[int(c)] for c in torch.argmax(output, dim=1)[0]] )\n",
    "        loss = criterion(output.view(-1, output_size), batch_targets.view(-1))  # Flatten the outputs and targets for CrossEntropyLoss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate accuracy\n",
    "            _, predicted_indices = torch.max(output, dim=2)  # Predicted characters\n",
    "\n",
    "            correct_predictions += (predicted_indices == batch_targets).sum().item()\n",
    "            total_predictions += batch_targets.size(0) * batch_targets.size(1)  # Total items in this batch\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100  # Convert to percentage\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    losses.append(avg_loss)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f'epoch{epoch}.pth'))\n",
    "    \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyperparameters for basic alphabet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 1000  # Length of each input sequence\n",
    "stride = 10            # Stride for creating sequences\n",
    "embedding_dim = 30     # Dimension of character embeddings\n",
    "hidden_size = 30     # Number of features in the hidden state of the RNN\n",
    "learning_rate = 200  # Learning rate for the optimizer\n",
    "num_epochs = 40        # Number of epochs to train\n",
    "batch_size = 64        # Batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss:\n",
    "\n",
    "![](./basic_alphabet_models/basic_alphabet_model_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyperparameters for War And Peace text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100  # Length of each input sequence\n",
    "stride = 10            # Stride for creating sequences\n",
    "embedding_dim = 30     # Dimension of character embeddings\n",
    "hidden_size = 500     # Number of features in the hidden state of the RNN\n",
    "learning_rate = 1e-3  # Learning rate for the optimizer\n",
    "num_epochs = 10        # Number of epochs to train\n",
    "batch_size = 64        # Batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss:  \n",
    "![](./warandpeace_models/warandpeace_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your loss\n",
    "\n",
    "The training loss of your model when trained with a simple sequence like `\"abcdefghijklmnopqrstuvwxyz\" * 100` should be extremely close to zero. If that's not the case, go back and fix your bugs ;)\n",
    "\n",
    "If you have acheived a training loss of 0 or extremley close to 0, then congratulations, lets move on to train your model with a bit more complicated sequence. That is our old favorite book, `warandpeace.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the `warandpeace.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #14\n",
    "sequence = read_file('warandpeace.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Follow the instructions\n",
    "\n",
    "1. Re-run Cell #5 to re-create character mappings for `warandpeace.txt`\n",
    "2. Re-run Cell #7 to re-initialize hyperparameters\n",
    "3. Re-run Cell #8 to split and create training and testing data with `warandpeace.txt` as your corpus\n",
    "4. Re-run Cell #9 to set up data loaders with `warandpeace.txt` data\n",
    "5. Re-run Cell #12 to re-initialize a new model object (maybe ask yourself why can't you use the previous model that was trained on the simple `\"abc...\"` corpus)\n",
    "6. Re-run Cell #13 to train the new model with `warandpeace.txt` data.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "After training, we evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Cell #15\n",
    "avg_loss = 0\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    #TODO: Write the testing loop for your trained model by refering to the training loop code given to you above\n",
    "    total_loss, correct_predictions, total_predictions = 0, 0, 0\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch_idx, (batch_inputs, batch_targets) in tqdm(enumerate(test_loader), total=total_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "        output, hidden = model(batch_inputs, hidden)\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        loss = criterion(output.view(-1, output_size), batch_targets.view(-1))  # Flatten the outputs and targets for CrossEntropyLoss\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted_indices = torch.max(output, dim=2)  # Predicted characters\n",
    "\n",
    "        correct_predictions += (predicted_indices == batch_targets).sum().item()\n",
    "        total_predictions += batch_targets.size(0) * batch_targets.size(1)  # Total items in this batch\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100  # Convert to percentage\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with the Trained Model\n",
    "\n",
    "In this part of the assignment, your task is to implement the `generate_text` function, which uses a trained RNN model to generate text character-by-character, continuing from a given input. The function will produce an extended sequence by repeatedly predicting and appending the next character to the input.\n",
    "\n",
    "### What the function is supposed to do?\n",
    "\n",
    "1. Take an initial input text of length `n` from the user, convert it into indices using a predefined vocabulary (char_to_idx).\n",
    "2. Use a trained model to predict the next character in the sequence.\n",
    "3. Append the predicted character to the input, extend the input sequence, and repeat the process until `k` additional characters are generated.\n",
    "4. Return the generated text, including the original input and the newly predicted characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Now you can generate text.\n",
      "Generated text: Due to light rain and fog that morning, the Los Angeles Police Department helicopters and most other air traffic were stationed. why are you sharp to anyone.how do you know why he added whom she was splint something to the campaign.the whole country.who served as to whom he had drive up a whole of t the troops to the picket like him. then he might take care and great moment, and that the man appeal condition of the most interrupted him at the contrary interverias to see the command and went up to the hill, but i have to come and would have been a sign all it is as a visitorsalsk and his arms face, and bent his\n"
     ]
    }
   ],
   "source": [
    "# This is Cell #16\n",
    "\n",
    "def sample_from_output(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample from the logits with temperature scaling.\n",
    "    logits: Tensor of shape [batch_size, vocab_size] (raw scores, before softmax)\n",
    "    temperature: a float controlling the randomness (higher = more random)\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling to logits (increase randomness with higher values)\n",
    "    scaled_logits = logits / temperature  # Scale the logits by temperature\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = F.softmax(scaled_logits, dim=1)\n",
    "    \n",
    "    # Sample from the probability distribution\n",
    "    sampled_idx = torch.multinomial(probabilities, 1)  # Sample one index from the probability distribution\n",
    "    return sampled_idx\n",
    "\n",
    "def generate_text(model, start_text, n, k, temperature=1.0):\n",
    "    \"\"\"\n",
    "        model: The trained RNN model used for character prediction.\n",
    "        start_text: The initial string of length `n` provided by the user to start the generation.\n",
    "        n: The length of the initial input sequence.\n",
    "        k: The number of additional characters to generate.\n",
    "        temperature: Optional\n",
    "        A scaling factor for randomness in predictions. Higher values (e.g., >1) make \n",
    "            predictions more random, while lower values (e.g., <1) make predictions more deterministic.\n",
    "            Default is 1.0.\n",
    "    \"\"\"    \n",
    "    start_text = start_text.lower()\n",
    "    #TODO: Implement the rest of the generate_text function\n",
    "    input_indices = [char_to_idx[c] for c in start_text]\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0)\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    generated_str = ''\n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            last_output = output[:, -1, :]\n",
    "            predicted_idx = sample_from_output(last_output, temperature=temperature)\n",
    "            predicted_char = idx_to_char[int(predicted_idx)]\n",
    "            generated_str += predicted_char\n",
    "            input_tensor = predicted_idx\n",
    "\n",
    "    return generated_str\n",
    "\n",
    "print(\"Training complete. Now you can generate text.\")\n",
    "\n",
    "#start_text = input(\"Enter the initial text (n characters, or 'exit' to quit): \")\n",
    "start_text = 'Due to light rain and fog that morning, the Los Angeles Police Department helicopters and most other air traffic were'\n",
    "\n",
    "n = len(start_text) \n",
    "#k = int(input(\"Enter the number of characters to generate: \"))\n",
    "k = 500\n",
    "#temperature_input = input(\"Enter the temperature value (1.0 is default, >1 is more random): \")\n",
    "#temperature = float(temperature_input) if temperature_input else 1.0\n",
    "with open(\"./warandpeace_models/char_to_idx.pkl\", 'rb') as f:\n",
    "    char_to_idx = pickle.load(f)\n",
    "\n",
    "with open(\"./warandpeace_models/idx_to_char.pkl\", 'rb') as f:\n",
    "    idx_to_char = pickle.load(f)\n",
    "temperature = 0.7\n",
    "model = CharRNN(input_size, hidden_size, output_size, embedding_dim)\n",
    "model.load_state_dict(torch.load(\"warandpeace_models/epoch9.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "completed_text = generate_text(model, start_text, n, k, temperature)\n",
    "\n",
    "print(f\"Generated text: {start_text + completed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefg\n"
     ]
    }
   ],
   "source": [
    "start_text = 'a'\n",
    "\n",
    "n = len(start_text) \n",
    "#k = int(input(\"Enter the number of characters to generate: \"))\n",
    "k = 500\n",
    "#temperature_input = input(\"Enter the temperature value (1.0 is default, >1 is more random): \")\n",
    "#temperature = float(temperature_input) if temperature_input else 1.0\n",
    "with open(\"./basic_alphabet_models/char_to_idx.pkl\", 'rb') as f:\n",
    "    char_to_idx = pickle.load(f)\n",
    "\n",
    "with open(\"./basic_alphabet_models/idx_to_char.pkl\", 'rb') as f:\n",
    "    idx_to_char = pickle.load(f)\n",
    "temperature = 0.999\n",
    "model = CharRNN(input_size, hidden_size, output_size, embedding_dim)\n",
    "model.load_state_dict(torch.load(\"basic_alphabet_models/epoch4.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "completed_text = generate_text(model, start_text, n, k, temperature)\n",
    "\n",
    "print(f\"Generated text: {start_text + completed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report section\n",
    "\n",
    "In your report, describe your experiments and observations when training the model with two datasets: (1) the sequence `\"abcdefghijklmnopqrstuvwxyz\" * 100` and (2) the text from `warandpeace.txt`. Include the final loss values for both datasets and discuss how the generated text differed between the two. Explain the impact of changing the `temperature` parameter on the text generation, and provide examples. Reflect on the challenges you faced, your thought process during implementation, and the key insights you gained about RNNs and sequence modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process for both models is similar, tweaking the hyper-parameters and see which works the best. The first sequence can easily produce a low loss as it's very simple, whereas the second text requires more training and tweaking. The final loss values for the first dataset is 0.0000 (both train and test), whereas the final train loss value for the second dataset is 1.2449, test loss is 0.1575. The text generated from the first model is always the same repetitive sequence of a-z no matter what the temperature is, whereas the second model becomes more readable and not repetitive at a moderate temperature. The temperature changes how static the text output is. A lower temperature means more static, making the text more likely to be repetitive, whereas a higher temperature is less static and generates more \"creative\" text. For example, a temperature of 0.0001 in the second model will produce a repetition of \"the countess and the countess was saying that he was a spirit of\", whereas a higher temperature will produce something like \"sign all it is as a visitorsalsk and his arms face, and bent his\", a combination of actual words and fake words, thus being more \"creative\".\n",
    "\n",
    "The biggest challenge is tweaking the hyper-parameters. Training takes a long time so tuning such parameters is very time-consuming. I learned how to implement RNN in pytorch, and it is interesting to see the code and the math come together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS383",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
